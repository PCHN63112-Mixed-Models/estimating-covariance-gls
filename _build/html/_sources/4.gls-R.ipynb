{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb5383df",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages(library(emmeans))\n",
    "suppressPackageStartupMessages(library(car))\n",
    "suppressPackageStartupMessages(library(effects))\n",
    "suppressPackageStartupMessages(library(nlme))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1ccab-2d89-4ccf-aca6-dbea239a0625",
   "metadata": {},
   "source": [
    "# Using GLS in `R`\n",
    "In the previous part of this lesson, we discussed the theory behind GLS. Now we turn to how GLS is implemented within `R`. We saw some examples of using the `gls()` function from `nlme` last semester. At that point, we only focused on the use of the `weights=` argument with different variance structures (e.g. `varIdent()`, `varPower()` etc.). However, there is also a `correlation=` argument that similarly takes a number of pre-specified correlation structures. We can use these two arguments together to form a final variance-covariance matrix that consists of *both* correlation and heterogenous variance groups. In this part of the lesson, we will explore `gls()` further using a simple repeated measures one-way ANOVA model. We will see examples of using `gls()` on more complex datasets in the workshop this week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee497ce0",
   "metadata": {},
   "source": [
    "## The GLS One-way ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a6eea",
   "metadata": {},
   "source": [
    "### The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b85d30",
   "metadata": {},
   "source": [
    "### The `gls()` Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ceafdb",
   "metadata": {},
   "source": [
    "### Checking Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b39d696",
   "metadata": {},
   "source": [
    "### Visualising the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe6a82",
   "metadata": {},
   "source": [
    "## Inference Using `gls()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9f7b9",
   "metadata": {},
   "source": [
    "### Coefficient Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e799c",
   "metadata": {},
   "source": [
    "### Omnibus Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256628d",
   "metadata": {},
   "source": [
    "### Follow-up Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dec075",
   "metadata": {},
   "source": [
    "## Viewing the Covariance Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9dd2d9",
   "metadata": {},
   "source": [
    "## Comparing Covariance Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbca1b1",
   "metadata": {},
   "source": [
    "[^weights-foot]: This is why the argument in `gls()` was `weights=`.\n",
    "\n",
    "[^corfunc-foot]: You can look up descriptions of all of these using `?corClasses` at the prompt. \n",
    "\n",
    "[^white-foot]: This is sometimes known as *whitening* the data. This is a term you may come across in the neuroimaging literature, particularly in relation to how fMRI is analysed.\n",
    "\n",
    "[^emmeans-foot]: The `mode=` option has been set to `df.error` so that the reported test matches the table from `summary()`. `emmeans` actually has some better ways of adjusting the degrees of freedom to accommodate the uncertainty in estimating $\\boldsymbol{\\Sigma}$, but this is a complication we will leave to one side for now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
