{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde1ccab-2d89-4ccf-aca6-dbea239a0625",
   "metadata": {},
   "source": [
    "# Generalised Least Squares (GLS)\n",
    "In the previous parts of this lesson, we discussed the idea of using more flexible covariance structures to model repeated measures data. Although desirable from a theoretical perspective, we saw that this causes problems in practice when it came to applying the classical inferential framework. So, we already start with a degree of *tension* between our model and inference based on that model. To begin with, however, we will put that tension to one side and focus only on the *model*. From this perspective, our aim is *not* to calculate a $p$-value or a confidence interval, it is to build something that can accurately capture the *data-generating process*. We will worry about inference later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c177b38a",
   "metadata": {},
   "source": [
    "## Why GLS?\n",
    "Although our general theme on this part of the unit is *mixed-effects models*, we are not quite there yet. So far, our journey has been to examine the *traditional* methods of capturing repeated measurements, in the form of the *paired $t$-test* and *repeated measures ANOVA*. Our conclusion was that these methods suffer from two problematic quirks that we would like to address:\n",
    "\n",
    "1. They require a *partitioning* of the error within a framework that does not allow this, leading to the awkward inclusion of error terms (e.g. `subject`) within the mean structure, and the manual assignment of numerators and denominators across multiple ANOVA tables.\n",
    "2. They make very *simplistic* assumptions about the covariance structure that are unlikely to hold in reality. \n",
    "\n",
    "Although mixed-effects models do address *both* of these issues (as we will see later), so too does a much more straightforward approach that we are already familiar with: Generalised Least Squares (GLS). As such, GLS presents a more logical starting point for addressing the issues we have identified. Indeed, this is useful because solving the two points above is *not* the main justification for using mixed-effects models. In fact, the justification is *not* that mixed-effects are better than GLS for every dataset, it is that mixed-effects are generally *more flexible* for complex data structures. However, this flexibility also comes at a price. So, there are times when GLS may actually be the better solution and we should not default to *mixed-effects* whenever we simply want a more general covariance structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb16c4",
   "metadata": {},
   "source": [
    "## GLS Theory\n",
    "We previously came across GLS in the context of allowing different variances for different groups of data in ANOVA-style models. This was motivated as a way of lifting the assumption of *homogeneity of variance*. However, GLS is a much more general technique. To see this, note that the probability model for GLS is\n",
    "\n",
    "$$\n",
    "\\mathbf{y} \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}\\right),\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\Sigma}$ can take on *any structure*. In other words, GLS has exactly the same probability model as the normal linear model, except that it allows for a flexible specification of the variance-covariance matrix. Importantly, this is done *without* an explicit partitioning of the error. So, a GLS version of the one-way repeated-measures ANOVA model would be\n",
    "\n",
    "$$\n",
    "y_{ij} = \\mu + \\alpha_{j} + \\epsilon_{ij},\n",
    "$$\n",
    "\n",
    "for subject $i$ in repeated measures condition $j$. Notice that there is *no* subject-specific term here. Indeed, the mean structure is *identical* to an independent one-way ANOVA model. This is sensible, given that we know that correlation does not affect the *mean function*. So, the difference here lies in the *variance function*. Under GLS, the *vector* of errors is assumed\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\epsilon} \\sim \\mathcal{N}\\left(\\mathbf{0},\\boldsymbol{\\Sigma}\\right)\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\Sigma}$ can take on any structure that can be estimated from the data. For instance, we could specify a block-diagonal matrix that was unstructured within each subject, and 0 everywhere else. We can express the block associated with subject $i$ as \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\epsilon}_{i} \\sim \\mathcal{N}\\left(\\mathbf{0},\\boldsymbol{\\Sigma}_{i}\\right)\n",
    "$$\n",
    "\n",
    "which, for the example of 3 repeated measurements per-subject, would expanded to\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\epsilon_{i1} \\\\\n",
    "    \\epsilon_{i2} \\\\\n",
    "    \\epsilon_{i3} \\\\\n",
    "\\end{bmatrix}\n",
    "\\sim \\mathcal{N}\\left(\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "\\end{bmatrix}, \n",
    "\\begin{bmatrix}\n",
    "    \\sigma^{2}_{1} & \\sigma_{12}    & \\sigma_{13}    \\\\\n",
    "    \\sigma_{12}    & \\sigma^{2}_{2} & \\sigma_{23}    \\\\\n",
    "    \\sigma_{13}    & \\sigma_{23}    & \\sigma^{2}_{3} \\\\\n",
    "\\end{bmatrix}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Crucially, there is no restriction on the form that $\\boldsymbol{\\Sigma}_{i}$ takes. So there is no need for this to be *compound symmetric* or *spherical*. We can let there be *any* correlation between the repeated measurement conditions, as well as *different variances* for each condition. This is because the covariance structure is formed *directly* and is an *explicit* part of the model. This is in direct opposition to the repeated measures ANOVA, where the covariance structure is simply *implied* and is formed *indirectly* from the partitioned errors. So, we can already see two key advantages here: we do not need to partition the error and include messy `subject` terms *and* we do not need to make a restrictive assumption about the correlational structure. \n",
    "\n",
    "Another key advantage is that the standard errors in a GLS model are formed *using $\\boldsymbol{\\Sigma}$*. This means that the correlational structure is *automatically* taken into account when calculating uncertainty. This is true of the standard errors of the parameter estimates, linear contrasts of the parameter estimates *and* ANOVA-style omnibus tests. This means the correct error term can be *automatically* determined for each test and we do not need multiple ANOVA tables with manual assignment of denominators. The model will take care of it all for us. \n",
    "\n",
    "So, taking all these points together, GLS would appear to solve all the issues we had previously with the repeated measures ANOVA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eba5f3",
   "metadata": {},
   "source": [
    "### How Does GLS Work?\n",
    "In order to get a sense of how GLS works, we need to start with the unrealistic assumption that we *know* $\\boldsymbol{\\Sigma}$. In other words, we know *exactly* what the population variances and covariances are between the repeated measures conditions. So, the only unknowns in our model are the parameters of the mean function. Although fanciful, we have seen this sort of thing before and is an approach typical of how these sorts of problems are addressed. We start with some simplifying assumptions to allow us to calculate what we need. We then see what happens when those assumptions are *lifted*. \n",
    "\n",
    "So, let us work through what happens when we *know* $\\boldsymbol{\\Sigma}$ precisely. In this situation, GLS is nothing more than a *transformation* of the model and the data. This is based on the idea that it is possible to find a matrix called $\\mathbf{W}$ which can be used to *pre-multiply* the data and the model to *remove* the covariance structure. This pre-multiplication is then equivalent to pre-multiplying the *errors*, which then removes all correlation and variance differences. For example, returning to subject $i$ from the one-way ANOVA example above, we originally had\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left(\\boldsymbol{\\epsilon}_{i}\\right) = \n",
    "\\begin{bmatrix}\n",
    "    \\sigma^{2}_{1} & \\sigma_{12}    & \\sigma_{13}    \\\\\n",
    "    \\sigma_{12}    & \\sigma^{2}_{2} & \\sigma_{23}    \\\\\n",
    "    \\sigma_{13}    & \\sigma_{23}    & \\sigma^{2}_{3} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "If we now correct both the *data* and the *model*, the new errors are equivalent to a *transformed* version of the form\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\epsilon}_{i}^{\\star} = \\mathbf{W}_{i} \\times \\boldsymbol{\\epsilon}_{i},\n",
    "$$\n",
    "\n",
    "which has the following covariance structure\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left(\\boldsymbol{\\epsilon}_{i}^{\\star}\\right) = \n",
    "\\begin{bmatrix}\n",
    "    \\sigma^{2} & 0          & 0          \\\\\n",
    "    0          & \\sigma^{2} & 0          \\\\\n",
    "    0          & 0          & \\sigma^{2} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "So, the basic idea is that if we *know* the covariance structure, we can therefore *remove it* from the data. Once we have done that, we have independent data and can simply go back to using the normal linear model. This is therefore a very *clean* and *simple* solution to the problem. If the covariance structure is what is causing us issues, we just *remove* it and then carry on as normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a763fa",
   "metadata": {},
   "source": [
    "## Feasible GLS (FGLS)\n",
    "Of course, the problem with the above process is that it is predicted on *knowing* $\\boldsymbol{\\Sigma}$. So, what happens in the more *realistic* scenario when we *do not know* $\\boldsymbol{\\Sigma}$? Well, we have to *estimate* it from the data. This is known as *feasible* GLS (FGLS), to make it distinct from the *true*, but ultimately impractical, theoretical GLS approach. This estimation can be performed using restricted maximum likelihood (REML), which is able to simultaneously find the parameters of the covariance structure and the parameters of the mean function that make the data most probable. So *practically* we are easily able to replace the true covariance structure $\\boldsymbol{\\Sigma}$ with an estimate $\\hat{\\boldsymbol{\\Sigma}}$.\n",
    "\n",
    "### Inferential Constraints\n",
    "However, we know from previous discussions the impact this has. Just to reiterate:\n",
    "\n",
    "- Because the removal of the covariance structure will be *imperfect*, the scaled $\\chi^{2}(\\nu)$ distribution is no longer applicable to describe the sampling distribution of the variance terms.\n",
    "- This means that the sampling distribution of the standard errors is no longer known.\n",
    "- Without a known sampling distribution, the null distribution of the tests statistics are also not known, meaning there is no way to calculate $p$-values and confidence intervals. \n",
    "- Without the concept of the scaled $\\chi^{2}(\\nu)$, the concept of *degrees of freedom* also disappear as a simple quantification of uncertainty.\n",
    "\n",
    "The argument is that *asymptotically*, FGLS converges on regular GLS. So, if the sample size is big enough we can effectively treat $\\boldsymbol{\\Sigma}$ as known because the uncertainty will be negligible. In this sense, the purest view on FGLS is that it is a method that is *only applicable in large samples*. However, there is nothing to stop us from applying it in *small* samples. We just have to recognise that inferentially we are back to the choices discussed in the previous part of the lesson. So, in a small sample, we either:\n",
    "\n",
    "1. Ignore the problem and just treat FGLS as if it were GLS.\n",
    "2. Calculate everything asymptotically and then adjust our interpretation depending upon how comfortable we feel, given the current sample size.\n",
    "3. Use *effective degrees of freedom* to try and approximate the small sample behaviour of the test statistics.\n",
    "\n",
    "We will see which of these options are available in `R` in the next part of the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de5f18",
   "metadata": {},
   "source": [
    "### Covariance Constraints\n",
    "As well as understanding that the very process of estimating $\\boldsymbol{\\Sigma}$ causes problems, we also need to understand that we cannot have free reign to estimate any old covariance structure we like. In order to understand how FGLS is implemented, we need to recognise that some sort of *constraint* is always needed when estimating a variance-covariance matrix. This means that we typically must decide between a finite number of *pre-defined structures*, rather than allowing the *whole matrix* to be anything it wants to be. \n",
    "\n",
    "To see why, note that for a repeated measures experiment there are $nt \\times nt$ values in $\\boldsymbol{\\Sigma}$. The values above and below the diagonal are a mirror image, so the true number of unknown values is $\\frac{nt(nt + 1)}{2}$. So, if we had $n = 5$ subjects and $t = 3$ repeated measures, there would be $\\frac{15 \\times 16}{2} = 120$ unique values in $\\boldsymbol{\\Sigma}$. If we had a more typical samples size of say $n=50$, this balloons to $\\frac{150 \\times 151}{2} = 11,325$. If we allowed this matrix to be *completely unstructured*, there would be 11,325 values to estimate *just* for the covariance structure alone. Indeed, this is not really possible unless the amount of data we have *exceeds* the number of parameters. So, the data itself imposes a *constraint* on how unstructured the covariance matrix can actually be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0941fa",
   "metadata": {},
   "source": [
    "#### Blocked Covariance Structures\n",
    "Luckily, for many applications, we can make the simplifying assumption that $\\boldsymbol{\\Sigma}$ has a *block-diagonal structure*. So we assume that *within* a subject, the covariance matrix has some unknown form, but *between* subjects the values are all 0. This reflects the idea that we assume that the repeated measurements are *correlated* but the subjects are *independent*. Formally, we can express this as\n",
    "\n",
    "$$\n",
    "\\text{Var}\\left(\\boldsymbol{\\epsilon}\\right) = \n",
    "\\begin{bmatrix}\n",
    "    \\boldsymbol{\\Sigma}_{1} & \\mathbf{0}              & \\mathbf{0}              & \\cdots & \\mathbf{0}              \\\\\n",
    "    \\mathbf{0}              & \\boldsymbol{\\Sigma}_{2} & \\mathbf{0}              & \\cdots & \\mathbf{0}              \\\\\n",
    "    \\mathbf{0}              & \\mathbf{0}              & \\boldsymbol{\\Sigma}_{3} & \\cdots & \\mathbf{0}              \\\\\n",
    "    \\vdots                  & \\vdots                  & \\vdots                  & \\ddots & \\vdots                  \\\\          \n",
    "    \\mathbf{0}              & \\mathbf{0}              & \\mathbf{0}              & \\cdots & \\boldsymbol{\\Sigma}_{n} \\\\\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where each of the $\\mathbf{0}$ are matrices of 0's and the *dots* just mean \"continue the pattern along the direction indicated\". Under this scheme, each of the $\\boldsymbol{\\Sigma}_{i}$'s is $t \\times t$, giving $\\frac{t(t+1)}{2}$ unique elements to estimate. For 3 repeated measures, this give 6 values per-subject. For $n=5$ this is a total of 30 unknown values. For $n=50$, this is 300 values. So although this is *relatively* smaller, it is not a *small* number of values in any absolute sense. \n",
    "\n",
    "From a purely numeric perspective, the degree to which a unique structure could be estimate *individually* for each subject would depend upon how many *repeats* of each repeated measures condition there are. For instance, if we only had 1 value per-subject and per-repeated measurement, it would be *impossible* to calculate the variance of *one number*, or the correlation between *two numbers*. Even if this were possible, the amount of *uncertainty* we would have would be huge because we are not pooling information across subjects, we are treating each subject as unique. This would only be feasible if we had *a lot* of data for every subject. \n",
    "\n",
    "However, even if we *did* have enough data, there is a theoretical problem with this. Capturing a unique structure for each subject tells us nothing about the information that is *shared* across the whole group of subjects. Remember, individual idiosyncrasies are not what we are interested in. Inference is about what we can learn by *pooling* subjects to reach some *global truth*. We want to imagine each subject as a random drawn from a *single population*, conceptualised probabilistically as a multivariate normal distribution with a *single covariance structure*. We do not want to define a population that is unique to each subject. That tells us nothing about the group as a whole. As such, the only way that we can reach conclusions about the *group* of subjects is to assume that the individual covariance structures are *identical*. In other words, all subjects are drawn from the *same* distribution and we have\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}_{1} = \\boldsymbol{\\Sigma}_{2} = \\boldsymbol{\\Sigma}_{3} = \\dots = \\boldsymbol{\\Sigma}_{n}.\n",
    "$$\n",
    "\n",
    "This also has a computational advantage because there are only $\\frac{t(t+1)}{2}$ elements to estimate *irrespective* of the sample size. Indeed, a bigger sample size means *more data* for each element of $\\boldsymbol{\\Sigma}$, which will reduce the uncertainty. This is the behaviour we would expect from increasing $n$. \n",
    "\n",
    "In addition, we do not have to make the blocks of $\\boldsymbol{\\Sigma}$ completely *unconstrained*. If we simplify the structure further, we need even *less* data and can increase the precision with which the covariance elements can be estimated. We saw this previously with the repeated measures ANOVA. Even though $\\boldsymbol{\\Sigma}$ may have *hundreds* of values we *could* fill-in, if we assume the same compound symmetric structure within each subject, there are only *two* covariance parameters to be estimated: $\\sigma^{2}_{b}$ and $\\sigma^{2}_{w}$. The whole matrix can then be constructed using those two values alone. This is an example of *extreme simplification* that is rooted in making sure classical inference still work. However, even once we accept a more *approximate* inferential framework, there still remains a risk from making the covariance matrix too general, due to the number of additional parameters needed. The more we estimate from the data, the greater our uncertainty will become because each element of the covariance-matrix is supported by *less data*. So there is no free lunch here. Increased complexity always comes at a price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f7ccff",
   "metadata": {},
   "source": [
    "#### More General Covariance Structures\n",
    "Although typical of most repeated measures experiments, a block-diagonal structure us not always the most appropriate for correlated data. Although we will not dwell on this, there are types of data *other* than traditional repeated measurements that are correlated and can be analysed using FGLS. Perhaps the most obvious is *time-series* data. For instance, imagine that you are taking continuous measurements from a single subject using a technique such as *eye-tracking*, *EEG* or *fMRI*. For each subject, we have repeated measurements that number the *hundreds* or even *thousands*. For example, the plot below shows a time-series of measurements using fMRI of a single subject at a single point in the brain.\n",
    "\n",
    "...\n",
    "\n",
    "The idea is that the values at each point in the time-series are close enough together that they are correlated. However, this correlation decreases the further apart in time the points are. So, two points that are *next to each other* will be strongly correlated, whereas two points at *opposite ends* of the time-series will be effectively *uncorrelated*. There is therefore a *decrease in correlation over time*. This is known as an *autoregressive* correlation structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2801408b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}